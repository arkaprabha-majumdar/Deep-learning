{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenCV notebook - ©Arkaprabha Majumdar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #1. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks are essentially better algorithms than ML for non linearly separable data.\n",
    "\n",
    "The input data into NNs are almost specifically numerical.\n",
    "\n",
    "It is capable of learning complex, non- linear mappings from data to produce better classifications.\n",
    "\n",
    "Consists of three main layers:\n",
    "    \n",
    "    Input Layer\n",
    "    Hidden Layers\n",
    "    Output Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a neural network of 2 input nodes, a single hidden layer with 2 nodes, and 2 output nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter no. of input nodes : 2\n",
      "[[0.45255876]\n",
      " [0.72553979]\n",
      " [1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#input nodes\n",
    "inp_nodes=[]\n",
    "for i in range(int(input(prompt=\"Enter no. of input nodes : \"))):\n",
    "    inp_nodes.append(np.random.uniform(0,1))\n",
    "inp_nodes=np.array(inp_nodes)\n",
    "inp_nodes=np.append(inp_nodes,1).reshape((3,1))\n",
    "print(inp_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter no. of layers : 2\n",
      "[[[0.21160951 0.75384909]\n",
      "  [0.49526241 0.50229725]]\n",
      "\n",
      " [[0.71904786 0.23012301]\n",
      "  [0.03196981 0.7720544 ]]]\n"
     ]
    }
   ],
   "source": [
    "#Considering 2 hidden nodes in the single hidden layer\n",
    "weights = []\n",
    "n=int(input(\"Enter no. of layers : \"))\n",
    "for layers in range(n):\n",
    "    weight_layer=[]\n",
    "    for i in range(len(inp_nodes)-1):\n",
    "        add_weights=[]\n",
    "        for j in range(len(inp_nodes)-1):\n",
    "            add_weights.append(np.random.uniform(0,1))\n",
    "        weight_layer.append(add_weights)\n",
    "    weights.append(weight_layer)\n",
    "weights=np.array(weights)\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.71904786, 0.23012301],\n",
       "       [0.03196981, 0.7720544 ]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57731464]\n",
      " [0.91991064]]\n"
     ]
    }
   ],
   "source": [
    "bias=[]\n",
    "for layer in range(n):\n",
    "    bias.append(np.array([np.random.uniform(0,1) for i in range(len(inp_nodes)-1)]).reshape((2,1)))\n",
    "print(bias[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the activation function, we can consider sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation :-\n",
    "1. We first forward propagate the input values, x_i\n",
    "\n",
    "2. multiply with randomly selected weights w_i\n",
    "\n",
    "3. add biases, b_i to the above\n",
    "\n",
    "4. apply an activation function - sigmoid/ReLU\n",
    "\n",
    "5. Do this throughout the hidden layers (consider 1) with nodes h_i, and outputs O_i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented matrix : \n",
      " [[0.71904786 0.23012301 0.57731464]\n",
      " [0.03196981 0.7720544  0.91991064]]\n",
      "Net H : \n",
      " [[1.06968945]\n",
      " [1.49453505]]\n",
      "hidden node values : \n",
      " [[0.74453785]\n",
      " [0.81675798]]\n"
     ]
    }
   ],
   "source": [
    "#as per the bias trick, we can reduce our computation time by augmenting weights and bias\n",
    "augment_mat = np.column_stack((weights[1],bias[1]))\n",
    "print(\"Augmented matrix : \\n\",augment_mat)\n",
    "H=np.matmul(augment_mat,inp_nodes)\n",
    "print(\"Net H : \\n\",H) #net value of hidden nodes\n",
    "H_i = sigmoid(H)\n",
    "print(\"hidden node values : \\n\",H_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets make this into a function:\n",
    "def calculate_values(weights,bias):\n",
    "    nodes = inp_nodes\n",
    "    augment_mat=[]\n",
    "    Net_H=[]\n",
    "    H=[]\n",
    "    for i in range(n):\n",
    "        print(i)\n",
    "        augment_mat.append(np.column_stack((weights[i],bias[i])))\n",
    "        print(\"Augmented matrix : \\n\",augment_mat[i])\n",
    "        Net_H.append(np.matmul(augment_mat[i],nodes))\n",
    "        print(\"Net H : \\n\",Net_H[i])\n",
    "        H.append(sigmoid(Net_H[i]))\n",
    "        print(\"hidden node values : \\n\",H[i])\n",
    "        nodes = np.append(H[i].reshape((1,2)),1).reshape((3,1))\n",
    "        #print(\"aug shape:\",augment_mat[i].shape)\n",
    "        #print(\"H shape\",H[i].shape)\n",
    "    return(augment_mat,Net_H,H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Augmented matrix : \n",
      " [[0.21160951 0.75384909 0.23122393]\n",
      " [0.49526241 0.50229725 0.08797374]]\n",
      "Net H : \n",
      " [[0.87393718]\n",
      " [0.67654573]]\n",
      "hidden node values : \n",
      " [[0.70556428]\n",
      " [0.6629673 ]]\n",
      "1\n",
      "Augmented matrix : \n",
      " [[0.71904786 0.23012301 0.57731464]\n",
      " [0.03196981 0.7720544  0.91991064]]\n",
      "Net H : \n",
      " [[1.23721316]\n",
      " [1.45431422]]\n",
      "hidden node values : \n",
      " [[0.77507855]\n",
      " [0.81066151]]\n"
     ]
    }
   ],
   "source": [
    "augment_mat,Net_H,H=calculate_values(weights,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2],\n",
       "       [0.3]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = np.array([1.2 , 0.3]).reshape((2,1))\n",
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our first \"hidden node value\" are H_1 and H_2\n",
    "\n",
    "The other two are O_1 and O_2\n",
    "\n",
    "Let our target values:\n",
    "    T1 = 1.2\n",
    "    T2=0.3 (just random assumptions, nothing ideal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next, we move towards \n",
    "### Gradient Descent Optimization using BackPropagation.\n",
    "\n",
    "We shall use Mini-Batch GD, which combines the advantages of both Naive and Stochastic GD methods.\n",
    "\n",
    "We shall also use Loss function MSE (Mean Square Error), which is always a good,safe bet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BackPropagation :-\n",
    "1. Calculate the loss using loss function E_i = 0.5 * (T_i - O_i)^2\n",
    "\n",
    "    The (1/2) is to cancel the exponent on differentiation (has no effect on result)\n",
    "    \n",
    "2. Next, we calculate E_total = sum(E_i)\n",
    "\n",
    "3. Select a weight w_k coming from previous layer.\n",
    "\n",
    "    We need to figure out if changing w_k will result in decrease in loss in E_total\n",
    "    \n",
    "4. Using chain rule:\n",
    "        (d E_total / d w_k) = (dE_total / dO_k) * (dO_k / d net0_k) * (d net0_k / dw_k)\n",
    "        \n",
    "        dE_total / dO_k = (T_k - O_k)\n",
    "        \n",
    "        dO_k / dNetO_k = O_k * (O_k)\n",
    "        #NetO_k is the input to output node, before applying sigmoid\n",
    "        \n",
    "        dNetO_k / dw_k = (h_j) #the hidden node connected with \n",
    "        \n",
    "5. Then we select a learning parameter \"η\", which is to scale DOWN the value change in w_k.\n",
    "    Taking too large learning parameter will overshoot the globalmin.\n",
    "    \n",
    "6. new w_k = w_k + η * (dE_total / dw_k)\n",
    "\n",
    "7. Do this for all the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate MSE\n",
    "def mse(output,target):\n",
    "    return(0.5*((target - output)**2))\n",
    "\n",
    "def derivative(weights, output, target):\n",
    "    i=0\n",
    "    for weight in weights:\n",
    "        print(\"weights : \\n\",weight)\n",
    "        component1 = (output - target)\n",
    "        component2 = output[1-i] * (1 - output[1-i])\n",
    "        component3 = output[i]\n",
    "        print(\"derivative : \\n\",component1*component2*component3)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights : \n",
      " [0.71904786 0.23012301]\n",
      "derivative : \n",
      " [[[-0.06081672]\n",
      "  [ 0.036935  ]]\n",
      "\n",
      " [[-0.05226631]\n",
      "  [ 0.05196413]]]\n",
      "weights : \n",
      " [0.03196981 0.7720544 ]\n",
      "derivative : \n",
      " [[[-0.07961275]\n",
      "  [ 0.06574628]]\n",
      "\n",
      " [[-0.06841975]\n",
      "  [ 0.09249895]]]\n"
     ]
    }
   ],
   "source": [
    "updations=derivative(weights[1],H,target) #these are updations for w5,w6,w7,w8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
